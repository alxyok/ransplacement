{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook demonstrates how to use the dataset. A large set of features and labels is formed. A tensor basis neural network (TBNN) is created using Keras. The network is trained and evaluated on some sample points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset features and labels, creating a training/validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.81359e-03, -6.92728e-16, -4.81359e-03, ..., -1.51669e-04,\n",
       "         1.51669e-04, -4.28356e-18],\n",
       "       [ 1.49546e+00, -8.24203e-12, -1.49546e+00, ..., -3.84830e+00,\n",
       "         3.84830e+00, -5.77841e-12],\n",
       "       [ 5.64550e+00, -1.11657e-10, -5.64550e+00, ..., -1.63879e+01,\n",
       "         1.63879e+01, -7.97615e-11],\n",
       "       ...,\n",
       "       [ 5.64550e+00,  2.04367e-10, -5.64550e+00, ...,  1.16517e+01,\n",
       "        -1.16517e+01,  1.30885e-10],\n",
       "       [ 1.49546e+00,  1.91466e-11, -1.49546e+00, ...,  2.17726e+00,\n",
       "        -2.17726e+00,  6.02927e-12],\n",
       "       [ 4.81359e-03,  1.87914e-15, -4.81359e-03, ...,  1.08213e-04,\n",
       "        -1.08213e-04,  5.67056e-18]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(f\"/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1100_I1.npy\", allow_pickle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(None, dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1100_q.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 1]\n",
      "  [2 3]\n",
      "  [4 5]\n",
      "  [6 7]\n",
      "  [8 9]]]\n",
      "[[[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3119/2974018789.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(10).reshape(1, 5, 2)\n",
    "print(x)\n",
    "y = np.arange(10, 20).reshape(1, 2, 5)\n",
    "print(y)\n",
    "z = tf.keras.layers.Dot(axes=(1, 2))([x, y])\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features and labels from the dataset: kepsilon\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1100_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1150_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1250_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1300_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1350_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1400_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1500_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1600_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1800_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_2205_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_2400_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_2600_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_2900_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_3200_Tensors.npy\n",
      "(9216, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_PHLL_case_0p5_Tensors.npy\n",
      "(14751, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_PHLL_case_0p8_Tensors.npy\n",
      "(14750, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_PHLL_case_1p0_Tensors.npy\n",
      "(14751, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_PHLL_case_1p5_Tensors.npy\n",
      "(14751, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_BUMP_h20_Tensors.npy\n",
      "(71103, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_BUMP_h26_Tensors.npy\n",
      "(70937, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_BUMP_h31_Tensors.npy\n",
      "(70951, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_BUMP_h42_Tensors.npy\n",
      "(70971, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_CNDV_12600_Tensors.npy\n",
      "(98700, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_CNDV_20580_Tensors.npy\n",
      "(183712, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_CBFS_13700_Tensors.npy\n",
      "(37089, 10, 3, 3)\n",
      "Shape of basis tensor array: (791490, 10, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1100_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1150_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1250_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1300_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1350_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1400_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1500_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1600_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_1800_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_2205_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_2400_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_2600_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_2900_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_DUCT_3200_I1.npy\n",
      "(9216, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_PHLL_case_0p5_I1.npy\n",
      "(14751, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_PHLL_case_0p8_I1.npy\n",
      "(14750, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_PHLL_case_1p0_I1.npy\n",
      "(14751, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_PHLL_case_1p5_I1.npy\n",
      "(14751, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_BUMP_h20_I1.npy\n",
      "(71103, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_BUMP_h26_I1.npy\n",
      "(70937, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_BUMP_h31_I1.npy\n",
      "(70951, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_BUMP_h42_I1.npy\n",
      "(70971, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_CNDV_12600_I1.npy\n",
      "(98700, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_CNDV_20580_I1.npy\n",
      "(183712, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/kepsilon/kepsilon_CBFS_13700_I1.npy\n",
      "(37089, 47)\n",
      "Shape of invariant features array: (791490, 47)\n",
      "Shape of combined features array: (791490, 47)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_1100_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_1150_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_1250_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_1300_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_1350_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_1400_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_1500_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_1600_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_1800_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_2205_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_2400_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_2600_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_2900_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/DUCT_3200_b.npy\n",
      "(9216, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/PHLL_case_0p5_b.npy\n",
      "(14751, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/PHLL_case_0p8_b.npy\n",
      "(14750, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/PHLL_case_1p0_b.npy\n",
      "(14751, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/PHLL_case_1p5_b.npy\n",
      "(14751, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/BUMP_h20_b.npy\n",
      "(71103, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/BUMP_h26_b.npy\n",
      "(70937, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/BUMP_h31_b.npy\n",
      "(70951, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/BUMP_h42_b.npy\n",
      "(70971, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/CNDV_12600_b.npy\n",
      "(98700, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/CNDV_20580_b.npy\n",
      "(183712, 3, 3)\n",
      "/home/jupyter/momentum/ml-turbulence-dataset/labels/CBFS_13700_b.npy\n",
      "(37089, 3, 3)\n",
      "Shape of DNS/LES labels array: (791490, 6)\n",
      " \n",
      "Training features:\n",
      "(633192, 47)\n",
      "Training tensor basis:\n",
      "(633192, 10, 3, 3)\n",
      "Training labels:\n",
      "(633192, 6)\n",
      " \n",
      "Validation features:\n",
      "(158298, 47)\n",
      "Validation tensor basis:\n",
      "(158298, 10, 3, 3)\n",
      "Validation labels:\n",
      "(158298, 6)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Loading data - select which cases to include in the training/validation set (commented out cases are held out)\n",
    "cases = ['DUCT_1100',\n",
    "         'DUCT_1150',\n",
    "         'DUCT_1250',\n",
    "         'DUCT_1300',\n",
    "         'DUCT_1350',\n",
    "         'DUCT_1400',\n",
    "         'DUCT_1500',\n",
    "         'DUCT_1600',\n",
    "         'DUCT_1800',\n",
    "         #'DUCT_2000',\n",
    "         'DUCT_2205',\n",
    "         'DUCT_2400',\n",
    "         'DUCT_2600',\n",
    "         'DUCT_2900',\n",
    "         'DUCT_3200',\n",
    "         #'DUCT_3500',\n",
    "         'PHLL_case_0p5',\n",
    "         'PHLL_case_0p8',\n",
    "         'PHLL_case_1p0',\n",
    "         #'PHLL_case_1p2',\n",
    "         'PHLL_case_1p5',\n",
    "         'BUMP_h20',\n",
    "         'BUMP_h26',\n",
    "         'BUMP_h31',\n",
    "         #'BUMP_h38',\n",
    "         'BUMP_h42',\n",
    "         'CNDV_12600',\n",
    "         'CNDV_20580',\n",
    "         'CBFS_13700'\n",
    "         ]\n",
    "\n",
    "#Convenient functions for loading dataset\n",
    "def loadCombinedArray(cases, field):\n",
    "    # data = np.concatenate([\n",
    "    #     np.load(f\"/home/jupyter/ml-turbulence-dataset/{dataset}/{dataset}_{case}_{field}.npy\", allow_pickle=True) \n",
    "    #     for case in cases])\n",
    "    l = []\n",
    "    for case in cases:\n",
    "        path = f\"/home/jupyter/momentum/ml-turbulence-dataset/{dataset}/{dataset}_{case}_{field}.npy\"\n",
    "        print(path)\n",
    "        arr = np.load(path, allow_pickle=True)\n",
    "        print(arr.shape)\n",
    "        l.append(arr)\n",
    "    return np.concatenate(l)\n",
    "\n",
    "def loadLabels(cases, field):\n",
    "    # data = np.concatenate([\n",
    "    #     np.load(f\"/home/jupyter/ml-turbulence-dataset/labels/{case}_{field}.npy\", allow_pickle=True) \n",
    "    #     for case in cases])\n",
    "    l = []\n",
    "    for case in cases:\n",
    "        path = f\"/home/jupyter/momentum/ml-turbulence-dataset/labels/{case}_{field}.npy\"\n",
    "        print(path)\n",
    "        arr = np.load(path, allow_pickle=True)\n",
    "        print(arr.shape)\n",
    "        l.append(arr)\n",
    "    return np.concatenate(l)\n",
    "\n",
    "# Select RANS model\n",
    "dataset = 'kepsilon' \n",
    "\n",
    "print('Loading features and labels from the dataset: '+ dataset)\n",
    "\n",
    "#Load the set of ten basis tensors (N,10,3,3), from Pope \"A more general effective-viscosity hypothesis\" (1975).\n",
    "Tensors = loadCombinedArray(cases, 'Tensors')\n",
    "print('Shape of basis tensor array: '+str(Tensors.shape))\n",
    "\n",
    "#Load the 47 invariants (N,47) used by Wu et al. \"Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework\" (2018)\n",
    "Invariants = loadCombinedArray(cases,'I1')\n",
    "print('Shape of invariant features array: '+str(Invariants.shape))\n",
    "\n",
    "#Load the additional scalars (N,5): \n",
    "#    q[:,0] = Ratio of excess rotation to strain rate,\n",
    "#    q[:,1] = Wall-distance based Reynolds number, \n",
    "#    q[:,2] = Ratio of turbulent time scale to mean strain time scale\n",
    "#    q[:,3] = Ratio of total Reynolds stress to 1/2 * normal Reynolds stress (TKE)\n",
    "# Scalars = loadCombinedArray(cases,'q')\n",
    "# print('Shape of scalar features array: '+str(Scalars.shape))\n",
    "\n",
    "#Combine the invariants and scalars to form a feature set\n",
    "# Features = np.column_stack((Invariants,Scalars))\n",
    "Features = Invariants\n",
    "print('Shape of combined features array: '+str(Features.shape))\n",
    "\n",
    "#Optional: remove outliers based on the number of standard deviations away from the mean. \n",
    "#Note: must be careful, as there are naturally large variations in flow features. Even a 5*stdev critera removes many valid near-wall points.\n",
    "def remove_outliers(Features):\n",
    "    stdev = np.std(Features,axis=0)\n",
    "    means = np.mean(Features,axis=0)\n",
    "    ind_drop = np.empty(0)\n",
    "    for i in range(len(Features[0,:])):\n",
    "        ind_drop = np.concatenate((ind_drop,np.where((Features[:,i]>means[i]+5*stdev[i]) | (Features[:,i]<means[i]-5*stdev[i]) )[0]))\n",
    "    return ind_drop.astype(int)\n",
    "\n",
    "outlier_removal_switch = 0\n",
    "if outlier_removal_switch == 1:\n",
    "    outlier_index = remove_outliers(Features)\n",
    "    print('Found '+str(len(outlier_index))+' outliers in the input feature set')\n",
    "    Features = np.delete(Features,outlier_index,axis=0)\n",
    "    Tensors = np.delete(Tensors,outlier_index,axis=0)\n",
    "    Labels = np.delete(Labels,outlier_index,axis=0)\n",
    "\n",
    "#Load the label set from DNS/LES:\n",
    "Labels = loadLabels(cases,'b')\n",
    "#If desired, reshape the 3x3 symmetric anisotropy tensor into a 1x6 vector\n",
    "Labels = np.delete(Labels.reshape((len(Labels),9)),[3,6,7],axis=1)\n",
    "print('Shape of DNS/LES labels array: '+str(Labels.shape))\n",
    "\n",
    "# Split the datasets into training and validation\n",
    "indices = np.arange(Features.shape[0])\n",
    "input_shape = Features.shape[1]\n",
    "\n",
    "x_train, x_val, y_train, y_val, ind_train, ind_val = train_test_split(Features, Labels, indices, test_size=0.2, random_state=10, shuffle=True)\n",
    "\n",
    "basis_train = Tensors[ind_train]\n",
    "basis_val = Tensors[ind_val]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "print(' ')\n",
    "print('Training features:')\n",
    "print(x_train.shape)\n",
    "print('Training tensor basis:')\n",
    "print(basis_train.shape)\n",
    "print('Training labels:')\n",
    "print(y_train.shape)\n",
    "print(' ')\n",
    "print('Validation features:')\n",
    "print(x_val.shape)\n",
    "print('Validation tensor basis:')\n",
    "print(basis_val.shape)\n",
    "print('Validation labels:')\n",
    "print(y_val.shape)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a tensor basis neural network (TBNN)\n",
    "The following model has 3 hidden layers, with 40 neurons each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([None, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 21:52:23.052948: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2021-11-25 21:52:23.219113: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "2021-11-25 21:52:23.226551: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6a5800d130 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-25 21:52:23.226597: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-11-25 21:52:23.230838: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([None, 10, 1, 1])\n",
      "TensorShape([None, 10, 3, 3])\n",
      "merge shape (None, 1, 1, 3, 3)\n",
      "TensorShape([None, 9, 1])\n",
      "TensorShape([None, 6])\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, 47)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Hidden1 (Dense)                 (None, 20)           960         input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Hidden2 (Dense)                 (None, 20)           420         Hidden1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Hidden3 (Dense)                 (None, 20)           420         Hidden2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gn (Dense)                      (None, 10)           210         Hidden3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Shape_for_dot_product (Reshape) (None, 10, 1, 1)     0           gn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "Tensor_input_layer (InputLayer) [(None, 10, 3, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Dot_product (Dot)               (None, 1, 1, 3, 3)   0           Shape_for_dot_product[0][0]      \n",
      "                                                                 Tensor_input_layer[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Shaped_output (Reshape)         (None, 9, 1)         0           Dot_product[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           Shaped_output[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           Shaped_output[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           Shaped_output[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1)            0           Shaped_output[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1)            0           Shaped_output[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1)            0           Shaped_output[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6)            0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,010\n",
      "Trainable params: 2,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "keras.backend.clear_session()\n",
    "\n",
    "#The model has two inputs, a set of input features with a learned mapping, and the tensor basis layer\n",
    "input_layer = keras.layers.Input(shape=(input_shape),name ='input_layer')\n",
    "input_tensor_basis = keras.layers.Input(shape=(10,3,3),name='Tensor_input_layer')\n",
    "\n",
    "#Hidden layer definition\n",
    "hidden1 = keras.layers.Dense(20,name='Hidden1', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(input_layer)\n",
    "hidden2 = keras.layers.Dense(20,name='Hidden2', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(hidden1)\n",
    "hidden3 = keras.layers.Dense(20,name='Hidden3', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(hidden2)\n",
    "\n",
    "#The layer of gn, which are coefficients for each of the ten Tn\n",
    "gn = keras.layers.Dense(10,name='gn', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-4), activation = \"selu\")(hidden3)\n",
    "tf.print(gn.shape)\n",
    "\n",
    "#Multiply the gn by Tn, with the output being the anisotropy tensor\n",
    "shaped = keras.layers.Reshape((10,1,1),name='Shape_for_dot_product')(gn)\n",
    "tf.print(shaped.shape)\n",
    "tf.print(input_tensor_basis.shape)\n",
    "merge = keras.layers.Dot(axes=1, name='Dot_product')([shaped,input_tensor_basis])\n",
    "tf.print(f'merge shape {merge.shape}')\n",
    "\n",
    "#Reshape the output anisotropy tensor, and trim out duplicate values (it is a symmetric matrix). The end result is a 6 component vector.\n",
    "shaped_output = keras.layers.Reshape((9,1),name='Shaped_output')(merge)\n",
    "tf.print(shaped_output.shape)\n",
    "trimmed_output1 = keras.layers.Lambda(lambda x : x[:,0])(shaped_output)\n",
    "trimmed_output2 = keras.layers.Lambda(lambda x : x[:,1])(shaped_output)\n",
    "trimmed_output3 = keras.layers.Lambda(lambda x : x[:,2])(shaped_output)\n",
    "trimmed_output4 = keras.layers.Lambda(lambda x : x[:,4])(shaped_output)\n",
    "trimmed_output5 = keras.layers.Lambda(lambda x : x[:,5])(shaped_output)\n",
    "trimmed_output6 = keras.layers.Lambda(lambda x : x[:,8])(shaped_output)\n",
    "merged_output = tf.keras.layers.Concatenate()([trimmed_output1,trimmed_output2,trimmed_output3,trimmed_output4,trimmed_output5,trimmed_output6])\n",
    "tf.print(merged_output.shape)\n",
    "\n",
    "model=keras.Model(inputs=[input_layer, input_tensor_basis], outputs=[merged_output])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate = 5E-4, clipnorm=1000)\n",
    "model.compile(optimizer,loss='mse',metrics=['mae', 'mse'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the tensor basis neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 633192 samples, validate on 158298 samples\n",
      "Epoch 1/1000\n",
      "633192/633192 [==============================] - 11s 17us/sample - loss: 4986.2144 - mae: 4.6733 - mse: 4986.1479 - val_loss: 1866.2248 - val_mae: 3.0001 - val_mse: 1866.1649\n",
      "Epoch 2/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 1137.9513 - mae: 2.3767 - mse: 1137.8928 - val_loss: 740.0033 - val_mae: 1.9312 - val_mse: 739.9443\n",
      "Epoch 3/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 581.3386 - mae: 1.7199 - mse: 581.2799 - val_loss: 695.3590 - val_mae: 1.6672 - val_mse: 695.3002\n",
      "Epoch 4/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 394.9613 - mae: 1.4398 - mse: 394.9028 - val_loss: 233.2145 - val_mae: 1.2756 - val_mse: 233.1563\n",
      "Epoch 5/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 269.1330 - mae: 1.2581 - mse: 269.0751 - val_loss: 361.1825 - val_mae: 1.3098 - val_mse: 361.1247\n",
      "Epoch 6/1000\n",
      "633192/633192 [==============================] - 9s 14us/sample - loss: 226.2093 - mae: 1.1307 - mse: 226.1518 - val_loss: 142.6041 - val_mae: 1.0444 - val_mse: 142.5468\n",
      "Epoch 7/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 162.1891 - mae: 1.0403 - mse: 162.1320 - val_loss: 103.7190 - val_mae: 0.9542 - val_mse: 103.6622\n",
      "Epoch 8/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 161.5357 - mae: 0.9902 - mse: 161.4790 - val_loss: 251.5413 - val_mae: 1.0515 - val_mse: 251.4847\n",
      "Epoch 9/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 110.1563 - mae: 0.8817 - mse: 110.0999 - val_loss: 195.7353 - val_mae: 0.9962 - val_mse: 195.6791\n",
      "Epoch 10/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 223.1263 - mae: 0.9082 - mse: 223.0702 - val_loss: 176.9864 - val_mae: 0.9042 - val_mse: 176.9305\n",
      "Epoch 11/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 152.9560 - mae: 0.8778 - mse: 152.9004 - val_loss: 121.2277 - val_mae: 0.8439 - val_mse: 121.1720\n",
      "Epoch 12/1000\n",
      "633192/633192 [==============================] - 9s 14us/sample - loss: 75.9197 - mae: 0.7650 - mse: 75.8642 - val_loss: 122.6884 - val_mae: 0.7769 - val_mse: 122.6330\n",
      "Epoch 13/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 64.7948 - mae: 0.7368 - mse: 64.7395 - val_loss: 102.3351 - val_mae: 0.7674 - val_mse: 102.2799\n",
      "Epoch 14/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 75.0212 - mae: 0.7168 - mse: 74.9661 - val_loss: 52.3738 - val_mae: 0.6821 - val_mse: 52.3189\n",
      "Epoch 15/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 80.7647 - mae: 0.7124 - mse: 80.7099 - val_loss: 84.5335 - val_mae: 0.7046 - val_mse: 84.4790\n",
      "Epoch 16/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 64.5651 - mae: 0.6330 - mse: 64.5106 - val_loss: 91.0020 - val_mae: 0.6499 - val_mse: 90.9478\n",
      "Epoch 17/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 46.5624 - mae: 0.5995 - mse: 46.5083 - val_loss: 53.5427 - val_mae: 0.5863 - val_mse: 53.4887\n",
      "Epoch 18/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 60.5691 - mae: 0.5810 - mse: 60.5153 - val_loss: 111.5872 - val_mae: 0.5688 - val_mse: 111.5336\n",
      "Epoch 19/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 51.3698 - mae: 0.5400 - mse: 51.3163 - val_loss: 16.3202 - val_mae: 0.4789 - val_mse: 16.2669\n",
      "Epoch 20/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 25.3829 - mae: 0.4809 - mse: 25.3296 - val_loss: 23.3448 - val_mae: 0.4742 - val_mse: 23.2917\n",
      "Epoch 21/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 18.7278 - mae: 0.4532 - mse: 18.6748 - val_loss: 156.3968 - val_mae: 0.5543 - val_mse: 156.3438\n",
      "Epoch 22/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 45.5713 - mae: 0.4860 - mse: 45.5185 - val_loss: 16.4729 - val_mae: 0.4331 - val_mse: 16.4202\n",
      "Epoch 23/1000\n",
      "633192/633192 [==============================] - 9s 14us/sample - loss: 23.4538 - mae: 0.4344 - mse: 23.4012 - val_loss: 27.3238 - val_mae: 0.4672 - val_mse: 27.2714\n",
      "Epoch 24/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 28.6113 - mae: 0.4343 - mse: 28.5591 - val_loss: 11.5078 - val_mae: 0.3668 - val_mse: 11.4558\n",
      "Epoch 25/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 14.6409 - mae: 0.3659 - mse: 14.5890 - val_loss: 9.9032 - val_mae: 0.3414 - val_mse: 9.8514\n",
      "Epoch 26/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 21.1111 - mae: 0.3727 - mse: 21.0595 - val_loss: 30.6134 - val_mae: 0.4106 - val_mse: 30.5619\n",
      "Epoch 27/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 11.8570 - mae: 0.3439 - mse: 11.8055 - val_loss: 15.8483 - val_mae: 0.3365 - val_mse: 15.7968\n",
      "Epoch 28/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 4.9214 - mae: 0.2881 - mse: 4.8701 - val_loss: 7.0342 - val_mae: 0.2595 - val_mse: 6.9830\n",
      "Epoch 29/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 5.5495 - mae: 0.2655 - mse: 5.4984 - val_loss: 19.6994 - val_mae: 0.3692 - val_mse: 19.6484\n",
      "Epoch 30/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 6.0309 - mae: 0.2712 - mse: 5.9800 - val_loss: 9.9165 - val_mae: 0.2575 - val_mse: 9.8656\n",
      "Epoch 31/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 3.9540 - mae: 0.2378 - mse: 3.9032 - val_loss: 5.7592 - val_mae: 0.2238 - val_mse: 5.7085\n",
      "Epoch 32/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 22.2898 - mae: 0.3009 - mse: 22.2392 - val_loss: 18.6900 - val_mae: 0.2919 - val_mse: 18.6396\n",
      "Epoch 33/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 4.6681 - mae: 0.2386 - mse: 4.6177 - val_loss: 5.0319 - val_mae: 0.2181 - val_mse: 4.9817\n",
      "Epoch 34/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 1.1477 - mae: 0.1991 - mse: 1.0976 - val_loss: 3.5224 - val_mae: 0.1888 - val_mse: 3.4724\n",
      "Epoch 35/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 1.3880 - mae: 0.1938 - mse: 1.3381 - val_loss: 5.1161 - val_mae: 0.1934 - val_mse: 5.0663\n",
      "Epoch 36/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 6.4853 - mae: 0.2340 - mse: 6.4356 - val_loss: 3.4076 - val_mae: 0.1837 - val_mse: 3.3581\n",
      "Epoch 37/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 1.5681 - mae: 0.1843 - mse: 1.5187 - val_loss: 2.4939 - val_mae: 0.1693 - val_mse: 2.4446\n",
      "Epoch 38/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 1.0013 - mae: 0.1693 - mse: 0.9521 - val_loss: 3.1013 - val_mae: 0.1799 - val_mse: 3.0523\n",
      "Epoch 39/1000\n",
      "633192/633192 [==============================] - 7s 11us/sample - loss: 2.1463 - mae: 0.1795 - mse: 2.0974 - val_loss: 13.8307 - val_mae: 0.3065 - val_mse: 13.7820\n",
      "Epoch 40/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 1.7661 - mae: 0.1703 - mse: 1.7176 - val_loss: 1.9540 - val_mae: 0.1509 - val_mse: 1.9057\n",
      "Epoch 41/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.8885 - mae: 0.1523 - mse: 0.8402 - val_loss: 2.3602 - val_mae: 0.1441 - val_mse: 2.3121\n",
      "Epoch 42/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 15.5568 - mae: 0.2073 - mse: 15.5087 - val_loss: 6.6003 - val_mae: 0.1819 - val_mse: 6.5524\n",
      "Epoch 43/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 4.0080 - mae: 0.1767 - mse: 3.9602 - val_loss: 4.9425 - val_mae: 0.1668 - val_mse: 4.8948\n",
      "Epoch 44/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 1.5707 - mae: 0.1549 - mse: 1.5230 - val_loss: 3.4783 - val_mae: 0.1363 - val_mse: 3.4308\n",
      "Epoch 45/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.9574 - mae: 0.1421 - mse: 0.9100 - val_loss: 3.1589 - val_mae: 0.1367 - val_mse: 3.1117\n",
      "Epoch 46/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 2.7998 - mae: 0.1583 - mse: 2.7527 - val_loss: 4.4501 - val_mae: 0.1428 - val_mse: 4.4032\n",
      "Epoch 47/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 1.7751 - mae: 0.1509 - mse: 1.7281 - val_loss: 1.8037 - val_mae: 0.1200 - val_mse: 1.7568\n",
      "Epoch 48/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.2858 - mae: 0.1176 - mse: 0.2391 - val_loss: 1.9291 - val_mae: 0.1234 - val_mse: 1.8825\n",
      "Epoch 49/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 1.2002 - mae: 0.1368 - mse: 1.1537 - val_loss: 1.9341 - val_mae: 0.1215 - val_mse: 1.8878\n",
      "Epoch 50/1000\n",
      "633192/633192 [==============================] - 7s 11us/sample - loss: 3.2214 - mae: 0.1615 - mse: 3.1752 - val_loss: 5.1833 - val_mae: 0.2394 - val_mse: 5.1371\n",
      "Epoch 51/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 2.5322 - mae: 0.1554 - mse: 2.4861 - val_loss: 1.8100 - val_mae: 0.1240 - val_mse: 1.7639\n",
      "Epoch 52/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.3135 - mae: 0.1159 - mse: 0.2676 - val_loss: 1.7245 - val_mae: 0.1217 - val_mse: 1.6787\n",
      "Epoch 53/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.3547 - mae: 0.1149 - mse: 0.3090 - val_loss: 1.0850 - val_mae: 0.1142 - val_mse: 1.0394\n",
      "Epoch 54/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.8499 - mae: 0.1229 - mse: 0.8044 - val_loss: 1.5910 - val_mae: 0.1682 - val_mse: 1.5456\n",
      "Epoch 55/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.2773 - mae: 0.1086 - mse: 0.2320 - val_loss: 0.8745 - val_mae: 0.0983 - val_mse: 0.8293\n",
      "Epoch 56/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.1641 - mae: 0.0974 - mse: 0.1191 - val_loss: 0.7156 - val_mae: 0.0957 - val_mse: 0.6708\n",
      "Epoch 57/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.1638 - mae: 0.0953 - mse: 0.1191 - val_loss: 0.7442 - val_mae: 0.0994 - val_mse: 0.6997\n",
      "Epoch 58/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.6611 - mae: 0.1108 - mse: 0.6169 - val_loss: 0.5752 - val_mae: 0.0911 - val_mse: 0.5311\n",
      "Epoch 59/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 2.1368 - mae: 0.1392 - mse: 2.0928 - val_loss: 0.6145 - val_mae: 0.1028 - val_mse: 0.5706\n",
      "Epoch 60/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.1614 - mae: 0.0925 - mse: 0.1176 - val_loss: 0.2876 - val_mae: 0.0897 - val_mse: 0.2439\n",
      "Epoch 61/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.1091 - mae: 0.0869 - mse: 0.0655 - val_loss: 0.2272 - val_mae: 0.0883 - val_mse: 0.1838\n",
      "Epoch 62/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.1208 - mae: 0.0842 - mse: 0.0775 - val_loss: 0.3077 - val_mae: 0.1042 - val_mse: 0.2645\n",
      "Epoch 63/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.2494 - mae: 0.0881 - mse: 0.2064 - val_loss: 0.2455 - val_mae: 0.0870 - val_mse: 0.2027\n",
      "Epoch 64/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.1777 - mae: 0.0855 - mse: 0.1351 - val_loss: 0.5850 - val_mae: 0.0992 - val_mse: 0.5424\n",
      "Epoch 65/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 4.0681 - mae: 0.1354 - mse: 4.0257 - val_loss: 0.9718 - val_mae: 0.1593 - val_mse: 0.9293\n",
      "Epoch 66/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.3992 - mae: 0.1087 - mse: 0.3567 - val_loss: 0.2267 - val_mae: 0.0941 - val_mse: 0.1844\n",
      "Epoch 67/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.2231 - mae: 0.0925 - mse: 0.1809 - val_loss: 0.9306 - val_mae: 0.1379 - val_mse: 0.8885\n",
      "Epoch 68/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.1329 - mae: 0.0857 - mse: 0.0908 - val_loss: 0.1942 - val_mae: 0.0819 - val_mse: 0.1523\n",
      "Epoch 69/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.3988 - mae: 0.0883 - mse: 0.3570 - val_loss: 1.2492 - val_mae: 0.1245 - val_mse: 1.2075\n",
      "Epoch 70/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.4723 - mae: 0.0953 - mse: 0.4307 - val_loss: 0.2386 - val_mae: 0.0802 - val_mse: 0.1971\n",
      "Epoch 71/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.2036 - mae: 0.0852 - mse: 0.1623 - val_loss: 0.1653 - val_mae: 0.0765 - val_mse: 0.1241\n",
      "Epoch 72/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0839 - mae: 0.0743 - mse: 0.0428 - val_loss: 0.1558 - val_mae: 0.0735 - val_mse: 0.1148\n",
      "Epoch 73/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0765 - mae: 0.0732 - mse: 0.0356 - val_loss: 0.1562 - val_mae: 0.0732 - val_mse: 0.1153\n",
      "Epoch 74/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0702 - mae: 0.0721 - mse: 0.0294 - val_loss: 0.1475 - val_mae: 0.0721 - val_mse: 0.1068\n",
      "Epoch 75/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0745 - mae: 0.0721 - mse: 0.0338 - val_loss: 0.1480 - val_mae: 0.0717 - val_mse: 0.1074\n",
      "Epoch 76/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0830 - mae: 0.0723 - mse: 0.0425 - val_loss: 0.1727 - val_mae: 0.0717 - val_mse: 0.1323\n",
      "Epoch 77/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0825 - mae: 0.0717 - mse: 0.0422 - val_loss: 0.1495 - val_mae: 0.0704 - val_mse: 0.1093\n",
      "Epoch 78/1000\n",
      "633192/633192 [==============================] - 9s 13us/sample - loss: 0.0764 - mae: 0.0707 - mse: 0.0364 - val_loss: 0.1434 - val_mae: 0.0699 - val_mse: 0.1034\n",
      "Epoch 79/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0896 - mae: 0.0706 - mse: 0.0497 - val_loss: 0.1516 - val_mae: 0.0704 - val_mse: 0.1119\n",
      "Epoch 80/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0751 - mae: 0.0692 - mse: 0.0355 - val_loss: 0.1389 - val_mae: 0.0674 - val_mse: 0.0994\n",
      "Epoch 81/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0674 - mae: 0.0680 - mse: 0.0280 - val_loss: 0.2082 - val_mae: 0.0725 - val_mse: 0.1689\n",
      "Epoch 82/1000\n",
      "633192/633192 [==============================] - 9s 14us/sample - loss: 0.1112 - mae: 0.0707 - mse: 0.0720 - val_loss: 0.1660 - val_mae: 0.0668 - val_mse: 0.1268\n",
      "Epoch 83/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0674 - mae: 0.0671 - mse: 0.0284 - val_loss: 0.1748 - val_mae: 0.0679 - val_mse: 0.1358\n",
      "Epoch 84/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0610 - mae: 0.0661 - mse: 0.0221 - val_loss: 0.1651 - val_mae: 0.0658 - val_mse: 0.1263\n",
      "Epoch 85/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0688 - mae: 0.0667 - mse: 0.0301 - val_loss: 0.2337 - val_mae: 0.0791 - val_mse: 0.1951\n",
      "Epoch 86/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.1008 - mae: 0.0691 - mse: 0.0623 - val_loss: 0.1903 - val_mae: 0.0712 - val_mse: 0.1518\n",
      "Epoch 87/1000\n",
      "633192/633192 [==============================] - 7s 11us/sample - loss: 0.0591 - mae: 0.0647 - mse: 0.0207 - val_loss: 0.1622 - val_mae: 0.0647 - val_mse: 0.1238\n",
      "Epoch 88/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0681 - mae: 0.0656 - mse: 0.0299 - val_loss: 0.1836 - val_mae: 0.0714 - val_mse: 0.1454\n",
      "Epoch 89/1000\n",
      "633192/633192 [==============================] - 9s 13us/sample - loss: 0.0904 - mae: 0.0671 - mse: 0.0523 - val_loss: 0.1594 - val_mae: 0.0642 - val_mse: 0.1214\n",
      "Epoch 90/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0881 - mae: 0.0667 - mse: 0.0502 - val_loss: 0.3031 - val_mae: 0.0791 - val_mse: 0.2653\n",
      "Epoch 91/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0718 - mae: 0.0653 - mse: 0.0340 - val_loss: 0.2026 - val_mae: 0.0646 - val_mse: 0.1648\n",
      "Epoch 92/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0543 - mae: 0.0627 - mse: 0.0166 - val_loss: 0.1884 - val_mae: 0.0629 - val_mse: 0.1508\n",
      "Epoch 93/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0598 - mae: 0.0633 - mse: 0.0223 - val_loss: 0.1990 - val_mae: 0.0630 - val_mse: 0.1616\n",
      "Epoch 94/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0646 - mae: 0.0639 - mse: 0.0272 - val_loss: 0.2204 - val_mae: 0.0624 - val_mse: 0.1831\n",
      "Epoch 95/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.0690 - mae: 0.0639 - mse: 0.0318 - val_loss: 0.2221 - val_mae: 0.0663 - val_mse: 0.1849\n",
      "Epoch 96/1000\n",
      "633192/633192 [==============================] - 7s 11us/sample - loss: 0.0600 - mae: 0.0627 - mse: 0.0229 - val_loss: 0.1926 - val_mae: 0.0615 - val_mse: 0.1556\n",
      "Epoch 97/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0591 - mae: 0.0619 - mse: 0.0222 - val_loss: 0.1915 - val_mae: 0.0618 - val_mse: 0.1547\n",
      "Epoch 98/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.0743 - mae: 0.0638 - mse: 0.0376 - val_loss: 0.2283 - val_mae: 0.0625 - val_mse: 0.1916\n",
      "Epoch 99/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0615 - mae: 0.0625 - mse: 0.0249 - val_loss: 0.2570 - val_mae: 0.0676 - val_mse: 0.2204\n",
      "Epoch 100/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0793 - mae: 0.0632 - mse: 0.0427 - val_loss: 0.2059 - val_mae: 0.0617 - val_mse: 0.1694\n",
      "Epoch 101/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0544 - mae: 0.0611 - mse: 0.0180 - val_loss: 0.5029 - val_mae: 0.0877 - val_mse: 0.4666\n",
      "Epoch 102/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0688 - mae: 0.0629 - mse: 0.0325 - val_loss: 0.2144 - val_mae: 0.0611 - val_mse: 0.1781\n",
      "Epoch 103/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0491 - mae: 0.0594 - mse: 0.0129 - val_loss: 0.2118 - val_mae: 0.0609 - val_mse: 0.1756\n",
      "Epoch 104/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0475 - mae: 0.0591 - mse: 0.0113 - val_loss: 0.2011 - val_mae: 0.0602 - val_mse: 0.1650\n",
      "Epoch 105/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0473 - mae: 0.0590 - mse: 0.0112 - val_loss: 0.2047 - val_mae: 0.0596 - val_mse: 0.1686\n",
      "Epoch 106/1000\n",
      "633192/633192 [==============================] - 7s 11us/sample - loss: 0.0474 - mae: 0.0589 - mse: 0.0113 - val_loss: 0.2037 - val_mae: 0.0598 - val_mse: 0.1677\n",
      "Epoch 107/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.0470 - mae: 0.0587 - mse: 0.0110 - val_loss: 0.1978 - val_mae: 0.0591 - val_mse: 0.1618\n",
      "Epoch 108/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0469 - mae: 0.0586 - mse: 0.0110 - val_loss: 0.2012 - val_mae: 0.0591 - val_mse: 0.1654\n",
      "Epoch 109/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0476 - mae: 0.0586 - mse: 0.0118 - val_loss: 0.1939 - val_mae: 0.0599 - val_mse: 0.1582\n",
      "Epoch 110/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.0483 - mae: 0.0585 - mse: 0.0127 - val_loss: 0.1915 - val_mae: 0.0591 - val_mse: 0.1558\n",
      "Epoch 111/1000\n",
      "633192/633192 [==============================] - 7s 11us/sample - loss: 0.0467 - mae: 0.0582 - mse: 0.0111 - val_loss: 0.1983 - val_mae: 0.0593 - val_mse: 0.1628\n",
      "Epoch 112/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.0466 - mae: 0.0580 - mse: 0.0111 - val_loss: 0.1961 - val_mae: 0.0585 - val_mse: 0.1606\n",
      "Epoch 113/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0466 - mae: 0.0579 - mse: 0.0111 - val_loss: 0.1901 - val_mae: 0.0599 - val_mse: 0.1547\n",
      "Epoch 114/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0461 - mae: 0.0577 - mse: 0.0108 - val_loss: 0.1893 - val_mae: 0.0581 - val_mse: 0.1541\n",
      "Epoch 115/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0467 - mae: 0.0578 - mse: 0.0115 - val_loss: 0.1816 - val_mae: 0.0589 - val_mse: 0.1465\n",
      "Epoch 116/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0461 - mae: 0.0575 - mse: 0.0110 - val_loss: 0.1876 - val_mae: 0.0582 - val_mse: 0.1525\n",
      "Epoch 117/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0457 - mae: 0.0573 - mse: 0.0107 - val_loss: 0.1849 - val_mae: 0.0589 - val_mse: 0.1499\n",
      "Epoch 118/1000\n",
      "633192/633192 [==============================] - 8s 12us/sample - loss: 0.0453 - mae: 0.0571 - mse: 0.0104 - val_loss: 0.1814 - val_mae: 0.0578 - val_mse: 0.1465\n",
      "Epoch 119/1000\n",
      "633192/633192 [==============================] - 7s 12us/sample - loss: 0.0457 - mae: 0.0572 - mse: 0.0108 - val_loss: 0.1793 - val_mae: 0.0593 - val_mse: 0.1446\n",
      "Epoch 120/1000\n",
      "633192/633192 [==============================] - 8s 13us/sample - loss: 0.0456 - mae: 0.0570 - mse: 0.0109 - val_loss: 0.1764 - val_mae: 0.0578 - val_mse: 0.1417\n"
     ]
    }
   ],
   "source": [
    "reduce_lr =tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"loss\",\n",
    "    factor=0.3,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=40,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit([x_train,basis_train], y_train, \n",
    "                    batch_size=1000,\n",
    "                    epochs=1000, \n",
    "                    validation_data = ([x_val,basis_val],y_val), \n",
    "                    verbose=1, \n",
    "                    callbacks=[\n",
    "                            early_stop,\n",
    "                            reduce_lr\n",
    "                            ]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([x_train,basis_train],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([x_val,basis_val],y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ind = np.random.randint(0,len(x_train),5)\n",
    "print('Picked 5 random indices: ' +str(rand_ind))\n",
    "\n",
    "for i in range(len(rand_ind)):\n",
    "    print('Index: '+str(rand_ind[i]))\n",
    "    print('Label anisotropy values: ')\n",
    "    print(y_train[rand_ind[i],:])\n",
    "    print('Model prediction: ')\n",
    "    print(model.predict([x_train[rand_ind[i],:].reshape(1,x_train.shape[1]),basis_train[rand_ind[i],:,:,:].reshape(1,10,3,3)]))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
