{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook demonstrates how to use the dataset. A large set of features and labels is formed. A tensor basis neural network (TBNN) is created using Keras. The network is trained and evaluated on some sample points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset features and labels, creating a training/validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Loading data - select which cases to include in the training/validation set (commented out cases are held out)\n",
    "cases = ['DUCT_1100',\n",
    "         'DUCT_1150',\n",
    "         'DUCT_1250',\n",
    "         'DUCT_1300',\n",
    "         'DUCT_1350',\n",
    "         'DUCT_1400',\n",
    "         'DUCT_1500',\n",
    "         'DUCT_1600',\n",
    "         'DUCT_1800',\n",
    "         #'DUCT_2000',\n",
    "         'DUCT_2205',\n",
    "         'DUCT_2400',\n",
    "         'DUCT_2600',\n",
    "         'DUCT_2900',\n",
    "         'DUCT_3200',\n",
    "         #'DUCT_3500',\n",
    "         'PHLL_case_0p5',\n",
    "         'PHLL_case_0p8',\n",
    "         'PHLL_case_1p0',\n",
    "         #'PHLL_case_1p2',\n",
    "         'PHLL_case_1p5',\n",
    "         'BUMP_h20',\n",
    "         'BUMP_h26',\n",
    "         'BUMP_h31',\n",
    "         #'BUMP_h38',\n",
    "         'BUMP_h42',\n",
    "         'CNDV_12600',\n",
    "         'CNDV_20580',\n",
    "         'CBFS_13700'\n",
    "         ]\n",
    "\n",
    "#Convenient functions for loading dataset\n",
    "def loadCombinedArray(cases,field):\n",
    "    data = np.concatenate([np.load('/kaggle/input/ml-turbulence-dataset/'+dataset+'/'+dataset+'_'+case+'_'+field + '.npy') for case in cases])\n",
    "    return data\n",
    "\n",
    "def loadLabels(cases,field):\n",
    "    data = np.concatenate([np.load('/kaggle/input/ml-turbulence-dataset/'+'labels/'+case+'_'+field + '.npy') for case in cases])\n",
    "    return data\n",
    "\n",
    "# Select RANS model\n",
    "dataset = 'kepsilon' \n",
    "\n",
    "print('Loading features and labels from the dataset: '+ dataset)\n",
    "\n",
    "#Load the set of ten basis tensors (N,10,3,3), from Pope \"A more general effective-viscosity hypothesis\" (1975).\n",
    "Tensors = loadCombinedArray(cases,'Tensors')\n",
    "print('Shape of basis tensor array: '+str(Tensors.shape))\n",
    "\n",
    "#Load the 47 invariants (N,47) used by Wu et al. \"Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework\" (2018)\n",
    "Invariants = loadCombinedArray(cases,'I')\n",
    "print('Shape of invariant features array: '+str(Invariants.shape))\n",
    "\n",
    "#Load the additional scalars (N,5): \n",
    "#    q[:,0] = Ratio of excess rotation to strain rate,\n",
    "#    q[:,1] = Wall-distance based Reynolds number, \n",
    "#    q[:,2] = Ratio of turbulent time scale to mean strain time scale\n",
    "#    q[:,3] = Ratio of total Reynolds stress to 1/2 * normal Reynolds stress (TKE)\n",
    "Scalars = loadCombinedArray(cases,'q')\n",
    "print('Shape of scalar features array: '+str(Scalars.shape))\n",
    "\n",
    "#Combine the invariants and scalars to form a feature set\n",
    "Features = np.column_stack((Invariants,Scalars))\n",
    "print('Shape of combined features array: '+str(Features.shape))\n",
    "\n",
    "#Optional: remove outliers based on the number of standard deviations away from the mean. \n",
    "#Note: must be careful, as there are naturally large variations in flow features. Even a 5*stdev critera removes many valid near-wall points.\n",
    "def remove_outliers(Features):\n",
    "    stdev = np.std(Features,axis=0)\n",
    "    means = np.mean(Features,axis=0)\n",
    "    ind_drop = np.empty(0)\n",
    "    for i in range(len(Features[0,:])):\n",
    "        ind_drop = np.concatenate((ind_drop,np.where((Features[:,i]>means[i]+5*stdev[i]) | (Features[:,i]<means[i]-5*stdev[i]) )[0]))\n",
    "    return ind_drop.astype(int)\n",
    "\n",
    "outlier_removal_switch = 0\n",
    "if outlier_removal_switch == 1:\n",
    "    outlier_index = remove_outliers(Features)\n",
    "    print('Found '+str(len(outlier_index))+' outliers in the input feature set')\n",
    "    Features = np.delete(Features,outlier_index,axis=0)\n",
    "    Tensors = np.delete(Tensors,outlier_index,axis=0)\n",
    "    Labels = np.delete(Labels,outlier_index,axis=0)\n",
    "\n",
    "#Load the label set from DNS/LES:\n",
    "Labels = loadLabels(cases,'b')\n",
    "#If desired, reshape the 3x3 symmetric anisotropy tensor into a 1x6 vector\n",
    "Labels = np.delete(Labels.reshape((len(Labels),9)),[3,6,7],axis=1)\n",
    "print('Shape of DNS/LES labels array: '+str(Labels.shape))\n",
    "\n",
    "# Split the datasets into training and validation\n",
    "indices = np.arange(Features.shape[0])\n",
    "input_shape = Features.shape[1]\n",
    "\n",
    "x_train, x_val, y_train, y_val, ind_train, ind_val = train_test_split(Features, Labels, indices, test_size=0.2, random_state=10, shuffle=True)\n",
    "\n",
    "basis_train = Tensors[ind_train]\n",
    "basis_val = Tensors[ind_val]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "print(' ')\n",
    "print('Training features:')\n",
    "print(x_train.shape)\n",
    "print('Training tensor basis:')\n",
    "print(basis_train.shape)\n",
    "print('Training labels:')\n",
    "print(y_train.shape)\n",
    "print(' ')\n",
    "print('Validation features:')\n",
    "print(x_val.shape)\n",
    "print('Validation tensor basis:')\n",
    "print(basis_val.shape)\n",
    "print('Validation labels:')\n",
    "print(y_val.shape)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a tensor basis neural network (TBNN)\n",
    "The following model has 3 hidden layers, with 40 neurons each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "keras.backend.clear_session()\n",
    "\n",
    "#The model has two inputs, a set of input features with a learned mapping, and the tensor basis layer\n",
    "input_layer = keras.layers.Input(shape=(input_shape),name ='input_layer')\n",
    "input_tensor_basis = keras.layers.Input(shape=(10,3,3),name='Tensor_input_layer')\n",
    "\n",
    "#Hidden layer definition\n",
    "hidden1 = keras.layers.Dense(20,name='Hidden1', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(input_layer)\n",
    "hidden2 = keras.layers.Dense(20,name='Hidden2', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(hidden1)\n",
    "hidden3 = keras.layers.Dense(20,name='Hidden3', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(hidden2)\n",
    "\n",
    "#The layer of gn, which are coefficients for each of the ten Tn\n",
    "gn = keras.layers.Dense(10,name='gn', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-4), activation = \"selu\")(hidden3)\n",
    "\n",
    "#Multiply the gn by Tn, with the output being the anisotropy tensor\n",
    "shaped = keras.layers.Reshape((10,1,1),name='Shape_for_dot_product')(gn)\n",
    "merge = keras.layers.Dot(axes=1, name='Dot_product')([shaped,input_tensor_basis])\n",
    "\n",
    "#Reshape the output anisotropy tensor, and trim out duplicate values (it is a symmetric matrix). The end result is a 6 component vector.\n",
    "shaped_output = keras.layers.Reshape((9,1),name='Shaped_output')(merge)\n",
    "trimmed_output1 = keras.layers.Lambda(lambda x : x[:,0])(shaped_output)\n",
    "trimmed_output2 = keras.layers.Lambda(lambda x : x[:,1])(shaped_output)\n",
    "trimmed_output3 = keras.layers.Lambda(lambda x : x[:,2])(shaped_output)\n",
    "trimmed_output4 = keras.layers.Lambda(lambda x : x[:,4])(shaped_output)\n",
    "trimmed_output5 = keras.layers.Lambda(lambda x : x[:,5])(shaped_output)\n",
    "trimmed_output6 = keras.layers.Lambda(lambda x : x[:,8])(shaped_output)\n",
    "merged_output = tf.keras.layers.Concatenate()([trimmed_output1,trimmed_output2,trimmed_output3,trimmed_output4,trimmed_output5,trimmed_output6])\n",
    "\n",
    "\n",
    "model=keras.Model(inputs=[input_layer, input_tensor_basis], outputs=[merged_output])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate = 5E-4, clipnorm=1000)\n",
    "model.compile(optimizer,loss='mse',metrics=['mae', 'mse'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the tensor basis neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr =tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"loss\",\n",
    "    factor=0.3,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=40,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit([x_train,basis_train], y_train, \n",
    "                    batch_size=1000,\n",
    "                    epochs=1000, \n",
    "                    validation_data = ([x_val,basis_val],y_val), \n",
    "                    verbose=1, \n",
    "                    callbacks=[\n",
    "                            early_stop,\n",
    "                            reduce_lr\n",
    "                            ]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([x_train,basis_train],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([x_val,basis_val],y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ind = np.random.randint(0,len(x_train),5)\n",
    "print('Picked 5 random indices: ' +str(rand_ind))\n",
    "\n",
    "for i in range(len(rand_ind)):\n",
    "    print('Index: '+str(rand_ind[i]))\n",
    "    print('Label anisotropy values: ')\n",
    "    print(y_train[rand_ind[i],:])\n",
    "    print('Model prediction: ')\n",
    "    print(model.predict([x_train[rand_ind[i],:].reshape(1,x_train.shape[1]),basis_train[rand_ind[i],:,:,:].reshape(1,10,3,3)]))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
